{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1647e9",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce785fc",
   "metadata": {},
   "source": [
    "a) \"Lucy plays with friends\" : (S (NP (NNP Lucy)) (VP (VBZ plays) (PP (IN with) (NP (NNS friends)))))\n",
    "\n",
    "\n",
    "b) \"This movie is careless and unfocused\" : (S (NP (DT This) (NN movie)) (VP (VBZ is) (ADJP (JJ careless) (CC and) (JJ unfocused))))\n",
    "\n",
    "\n",
    "c) Two possible parses for the sentence \"She buys a gift with gold\":\n",
    "\n",
    "1. (S (NP (PRP She)) (VP (VBZ buys) (NP (DT a) (NN gift) (PP (IN with) (NP (NN gold))))))\n",
    "\n",
    "2. (S (NP (PRP She)) (VP (VBZ buys) (NP (DT a) (NN gift) (PP (IN with) (NP (JJ gold))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3c55dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Obtaining dependency information for stanza from https://files.pythonhosted.org/packages/88/4f/064015f46172c860b02148db65acd67e4925900b426f66cd0f5729d1c0d1/stanza-1.6.1-py3-none-any.whl.metadata\n",
      "  Downloading stanza-1.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Obtaining dependency information for emoji from https://files.pythonhosted.org/packages/96/c6/0114b2040a96561fd1b44c75df749bbd3c898bf8047fb5ce8d7590d2dee6/emoji-2.8.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from stanza) (1.24.3)\n",
      "Collecting protobuf>=3.15.0 (from stanza)\n",
      "  Obtaining dependency information for protobuf>=3.15.0 from https://files.pythonhosted.org/packages/e6/db/7b2edc72807d45d72f9db42f3eb86ddaf37f9e55d923159b1dbfc9d835bc/protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from stanza) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from requests->stanza) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/pradaapss/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Downloading stanza-1.6.1-py3-none-any.whl (881 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, emoji, stanza\n",
      "Successfully installed emoji-2.8.0 protobuf-4.25.1 stanza-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa47b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16645027",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282cd4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:01:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3c326896954810bba7418611946dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1710019cb48b4af1bfcff0ba41f49255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/tokenize/combined.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319a0253a56042a89cdf8031f4eafefb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/pos/combined_charlm.pt:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c398e9f344024149a65afc3fb6039982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/lemma/combined_nocharlm.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf7829482e14041bdb79cb5121627b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/depparse/combined_charlm.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bf45fb448549e1871d359b4cd34a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/backward_charlm/1billion.pt:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd89b0f7c7864c3da5e956fa04818231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/forward_charlm/1billion.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbd29019ec64176a7d2cb8b02c09858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/pretrain/conll17.pt:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:02:10 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2023-11-20 22:02:10 INFO: Using device: cpu\n",
      "2023-11-20 22:02:10 INFO: Loading: tokenize\n",
      "2023-11-20 22:02:10 INFO: Loading: pos\n",
      "2023-11-20 22:02:10 INFO: Loading: lemma\n",
      "2023-11-20 22:02:10 INFO: Loading: depparse\n",
      "2023-11-20 22:02:10 INFO: Done loading processors!\n",
      "2023-11-20 22:02:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432d60fbafa74d71aa616c2727a08ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:02:11 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2023-11-20 22:02:11 INFO: Using device: cpu\n",
      "2023-11-20 22:02:11 INFO: Loading: tokenize\n",
      "2023-11-20 22:02:11 INFO: Loading: pos\n",
      "2023-11-20 22:02:11 INFO: Loading: lemma\n",
      "2023-11-20 22:02:11 INFO: Loading: depparse\n",
      "2023-11-20 22:02:11 INFO: Done loading processors!\n",
      "2023-11-20 22:02:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee8966bb2a94cffb7e5a8f14c8b0dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:02:12 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2023-11-20 22:02:12 INFO: Using device: cpu\n",
      "2023-11-20 22:02:12 INFO: Loading: tokenize\n",
      "2023-11-20 22:02:12 INFO: Loading: pos\n",
      "2023-11-20 22:02:12 INFO: Loading: lemma\n",
      "2023-11-20 22:02:12 INFO: Loading: depparse\n",
      "2023-11-20 22:02:12 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Lucy plays with friends Constituency Parse: ('Lucy', 2, 'nsubj')\n",
      "('plays', 0, 'root')\n",
      "('with', 4, 'case')\n",
      "('friends', 2, 'obl') \n",
      "==================================================\n",
      "\n",
      "Original Sentence: This movie is careless and unfocused Constituency Parse: ('This', 2, 'det')\n",
      "('movie', 4, 'nsubj')\n",
      "('is', 4, 'cop')\n",
      "('careless', 0, 'root')\n",
      "('and', 6, 'cc')\n",
      "('unfocused', 4, 'conj') \n",
      "==================================================\n",
      "\n",
      "Original Sentence: She buys a gift with gold Constituency Parse: ('She', 2, 'nsubj')\n",
      "('buys', 0, 'root')\n",
      "('a', 4, 'det')\n",
      "('gift', 2, 'obj')\n",
      "('with', 6, 'case')\n",
      "('gold', 2, 'obl') \n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_sentence(sentence):\n",
    "    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma,depparse', use_gpu=False)\n",
    "    doc = nlp(sentence)\n",
    "    return [(f\"Original Sentence: {sentence}\",\n",
    "             \"Constituency Parse:\",\n",
    "             sent.dependencies_string(),\n",
    "             \"\\n\" + \"=\" * 50 + \"\\n\") for sent in doc.sentences]\n",
    "\n",
    "sentences_to_process = [\"Lucy plays with friends\",\n",
    "                        \"This movie is careless and unfocused\",\n",
    "                        \"She buys a gift with gold\"]\n",
    "\n",
    "parsed_sentences = [result for sentence in sentences_to_process for result in process_sentence(sentence)]\n",
    "\n",
    "for result in parsed_sentences:\n",
    "    print(*result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc69b58",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f528c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the combined dataset: 35\n",
      "\n",
      "Elementary text of the first row:\n",
      "Poorer countries will be most affected\n",
      "by climate change in the next century.\n",
      "Sea levels will rise, there will be stronger\n",
      "cyclones, warmer days and nights, more\n",
      "rainfall, and larger and longer heatwaves,\n",
      "says a new report.\n",
      "\n",
      "Advanced text of the first row:\n",
      "Low-income countries will remain on the front\n",
      "line of human-induced climate change over the\n",
      "next century, experiencing gradual sea-level\n",
      "rises, stronger cyclones, warmer days and\n",
      "nights, more unpredictable rainfall, and larger\n",
      "and longer heatwaves, according to the most\n",
      "thorough assessment of the issue yet.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "climate_change = pd.read_csv('/Users/pradaapss/Desktop/Semester 3/CS 585 NLP/Assignment 5/climate_change .csv', encoding='ISO-8859-1')\n",
    "gangs = pd.read_csv('/Users/pradaapss/Desktop/Semester 3/CS 585 NLP/Assignment 5/Gangs.csv', encoding='ISO-8859-1')\n",
    "thatcher = pd.read_csv('/Users/pradaapss/Desktop/Semester 3/CS 585 NLP/Assignment 5/Thatcher.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Filter rows with \"Elementary\" parse\n",
    "climate_change = climate_change[climate_change['Elementary'].notnull()]\n",
    "gangs = gangs[gangs['Elementary'].notnull()]\n",
    "thatcher = thatcher[thatcher['Elementary'].notnull()]\n",
    "\n",
    "# Merge datasets\n",
    "combined_dataset = pd.concat([climate_change, gangs, thatcher], ignore_index=True)\n",
    "\n",
    "# Print the number of rows in the combined dataset\n",
    "print(\"Number of rows in the combined dataset:\", len(combined_dataset))\n",
    "\n",
    "# Print Elementary and Advanced texts for the first row\n",
    "print(\"\\nElementary text of the first row:\")\n",
    "print(combined_dataset.loc[0, 'Elementary'])\n",
    "print(\"\\nAdvanced text of the first row:\")\n",
    "print(combined_dataset.loc[0, 'Advanced'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036e824",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56d44402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841335c9151441dbb954ff351c1ec6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:15:41 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-11-20 22:15:42 INFO: File exists: /Users/pradaapss/stanza_resources/en/default.zip\n",
      "2023-11-20 22:15:46 INFO: Finished downloading models and saved to /Users/pradaapss/stanza_resources.\n",
      "2023-11-20 22:15:46 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f12b0d02dd4fefa6786562c9755efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:15:46 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-11-20 22:15:46 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2023-11-20 22:15:46 INFO: Using device: cpu\n",
      "2023-11-20 22:15:46 INFO: Loading: tokenize\n",
      "2023-11-20 22:15:46 INFO: Loading: pos\n",
      "2023-11-20 22:15:47 INFO: Loading: lemma\n",
      "2023-11-20 22:15:47 INFO: Loading: depparse\n",
      "2023-11-20 22:15:47 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elementary Summary:\n",
      "{'Average_Sentences': 3.2285714285714286, 'Average_Prepositional_Phrases': 6.2, 'Average_Other_Attribute_CC': 2.0285714285714285}\n",
      "\n",
      "Advanced Summary:\n",
      "{'Average_Sentences': 3.1142857142857143, 'Average_Prepositional_Phrases': 8.6, 'Average_Other_Attribute_CC': 2.2857142857142856}\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from typing import List\n",
    "\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize the Stanza pipeline for constituency parsing\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,pos,lemma,depparse', use_gpu=False)\n",
    "\n",
    "\n",
    "def count_prepositional_phrases(sentence):\n",
    "    # Count the number of prepositional phrases (rooted at 'case' or 'mark' dependency labels)\n",
    "    prepositional_phrases = [word.deprel for word in sentence.words if word.deprel in {'case', 'mark'}]\n",
    "    return len(prepositional_phrases)\n",
    "\n",
    "def count_attribute(sentences, attribute):\n",
    "    return sum(word.deprel == attribute for sent in sentences for word in sent.words)\n",
    "\n",
    "\n",
    "def analyze_text_properties(texts: List[str]) -> dict:\n",
    "    # Initialize counters\n",
    "    total_sentences = 0\n",
    "    total_prepositional_phrases = 0\n",
    "    total_other_attribute = 0  # Change this to the attribute you choose\n",
    "\n",
    "    # Process each text\n",
    "    for text in texts:\n",
    "        # Process the text using Stanza\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Calculate the number of sentences\n",
    "        total_sentences += len(doc.sentences)\n",
    "    \n",
    "         # Count prepositional phrases in each sentence\n",
    "        total_prepositional_phrases += sum(count_prepositional_phrases(sent) for sent in doc.sentences)\n",
    "\n",
    "        # Count occurrences of the chosen attribute in each sentence\n",
    "        total_other_attribute += count_attribute(doc.sentences, 'cc')\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_sentences = total_sentences / len(texts)\n",
    "    avg_prepositional_phrases = total_prepositional_phrases / len(texts)\n",
    "    avg_other_attribute = total_other_attribute / len(texts)\n",
    "\n",
    "    # Create data summary\n",
    "   \n",
    "    \n",
    "    summary = dict(\n",
    "    Average_Sentences=avg_sentences,\n",
    "    Average_Prepositional_Phrases=avg_prepositional_phrases,\n",
    "    Average_Other_Attribute_CC=avg_other_attribute,\n",
    "    )\n",
    "\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage with the data from Problem 3\n",
    "elementary_texts = combined_dataset['Elementary'].dropna().tolist()\n",
    "advanced_texts = combined_dataset['Advanced'].dropna().tolist()\n",
    "\n",
    "elementary_summary = analyze_text_properties(elementary_texts)\n",
    "advanced_summary = analyze_text_properties(advanced_texts)\n",
    "\n",
    "# Display the results\n",
    "print(\"Elementary Summary:\")\n",
    "print(elementary_summary)\n",
    "\n",
    "print(\"\\nAdvanced Summary:\")\n",
    "print(advanced_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fcabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dbeeed4",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c741cbe",
   "metadata": {},
   "source": [
    "The attribute I added in Problem 4 is the count of coordinating conjunctions (CC) in each text.\n",
    "\n",
    "Regarding its utility as an indicator of reading level, the count of coordinating conjunctions might offer insights into syntactic complexity. However, its direct correlation to reading level might be limited. In the Elementary texts, the average count of coordinating conjunctions per text is lower (2.028) compared to the Advanced texts (2.285). This indicates a slight increase in syntactic complexity in Advanced texts, but this alone might not be a robust indicator of reading level as it's a relatively subtle difference. Other factors such as vocabulary, sentence structure, and topic complexity should be considered for a more comprehensive evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49bb36",
   "metadata": {},
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d9dd1",
   "metadata": {},
   "source": [
    "To classify texts based on reading complexity and tailor them to students' proficiency levels, one useful approach I would suggest is to use the Naive Bayes classifier. Naive Bayes is a statistical method commonly employed for categorizing text.\n",
    "\n",
    "Here's how we can use Naive Bayes for this task:\n",
    "\n",
    "1. **Training the Model:**\n",
    "   - Gather a labeled dataset of texts, tagged with their respective complexity levels.\n",
    "   - Process the texts by breaking them into tokens and cleaning the data. This involves tasks such as removing stop words, punctuation, and applying stemming or lemmatization.\n",
    "   - Train the Naive Bayes classifier using the labeled data. The classifier learns the probability distribution of words associated with each complexity level.\n",
    "\n",
    "2. **Feature Representation:**\n",
    "   - Represent the texts as feature vectors. Techniques like bag-of-words can be used for this purpose. These methods capture the significance of words in distinguishing different complexity levels.\n",
    "\n",
    "3. **Predicting Complexity:**\n",
    "   - Preprocess new texts in a similar manner as the training data.\n",
    "   - Utilize the trained Naive Bayes classifier to predict the complexity level of a text based on the learned probabilities.\n",
    "\n",
    "4. **Adjusting for Accuracy:**\n",
    "   - Assess the model's performance using a separate validation dataset. Fine-tune hyperparameters or adjust features to enhance accuracy if necessary.\n",
    "\n",
    "5. **Implementing in the Tool:**\n",
    "   - Integrate the trained Naive Bayes classifier into the tool for automated text classification.\n",
    "   - Display the predicted complexity level alongside each text or use it to filter and recommend texts suitable for a student's proficiency level.\n",
    "\n",
    "By employing Naive Bayes in this manner, the tool becomes adept at categorizing texts into various complexity levels, enabling students to access materials aligned with their current English proficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bf189",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc8eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
