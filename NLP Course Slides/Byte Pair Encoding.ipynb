{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pc8MYXzH3m6w"
   },
   "source": [
    "# Demonstration of byte-pair encoding\n",
    "\n",
    "Code adapted from https://leimao.github.io/blog/Byte-Pair-Encoding/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1662520829916,
     "user": {
      "displayName": "Kai Shu",
      "userId": "12656867753077324183"
     },
     "user_tz": 300
    },
    "id": "TB6hg1LA3m6z"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1662520832230,
     "user": {
      "displayName": "Kai Shu",
      "userId": "12656867753077324183"
     },
     "user_tz": 300
    },
    "id": "nINAANJG3m60"
   },
   "outputs": [],
   "source": [
    "def get_vocab(word_gen):\n",
    "    \"\"\"Initialize vocabulary from corpus\n",
    "    \"\"\"\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for word in word_gen:\n",
    "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Calculate co-occurrence statistics for token bigrams\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"Merge two tokens and update the resulting vocabulary\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_tokens_from_vocab(vocab):\n",
    "    \"\"\"Recover tokens from tokenized vocabulary\n",
    "    \"\"\"\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aSWzY6JS3m61",
    "outputId": "d230ab54-fd9a-45b7-ec67-331deb1eb21e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Tokens Before BPE\n",
      "All tokens: dict_keys(['t', 'h', 'e', '</w>', 'f', 'u', 'l', 'o', 'n', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'p', 'm', 'k', 'x', 'w', 'b', 'z', 'q'])\n",
      "Number of tokens: 27\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab(map(lambda x: x.lower(),\n",
    "                      filter(lambda x: x.isalpha(), \n",
    "                             brown.words())))\n",
    "\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "print('==========')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IUB165YQ3m62"
   },
   "outputs": [],
   "source": [
    "def step_bpe(v):\n",
    "    pairs = get_stats(v)\n",
    "    if not pairs:\n",
    "        return v\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    v = merge_vocab(best, v)\n",
    "    return v, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GRCJZk543m63",
    "outputId": "92309e82-9856-4950-9c5d-961b3a3330e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('e', '</w>')\n",
      "All tokens: dict_keys(['t', 'h', 'e</w>', 'f', 'u', 'l', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'e', 'p', 'm', 'k', 'x', 'w', 'b', 'z', 'q'])\n",
      "Number of tokens: 28\n",
      "==========\n",
      "Iter: 1\n",
      "Best pair: ('t', 'h')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'e', 'p', 'm', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 29\n",
      "==========\n",
      "Iter: 2\n",
      "Best pair: ('s', '</w>')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd', 'j', 's', 'i', 'v', 'e', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 30\n",
      "==========\n",
      "Iter: 3\n",
      "Best pair: ('d', '</w>')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'v', 'e', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 31\n",
      "==========\n",
      "Iter: 4\n",
      "Best pair: ('t', '</w>')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 32\n",
      "==========\n",
      "Iter: 5\n",
      "Best pair: ('i', 'n')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 33\n",
      "==========\n",
      "Iter: 6\n",
      "Best pair: ('e', 'r')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'a', 'd</w>', 'j', 's', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 34\n",
      "==========\n",
      "Iter: 7\n",
      "Best pair: ('a', 'n')\n",
      "All tokens: dict_keys(['th', 'e</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'an', 'd</w>', 'j', 's', 'a', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 35\n",
      "==========\n",
      "Iter: 8\n",
      "Best pair: ('th', 'e</w>')\n",
      "All tokens: dict_keys(['the</w>', 'f', 'u', 'l', 't', 'o', 'n', '</w>', 'c', 'y', 'g', 'r', 'an', 'd</w>', 'j', 's', 'a', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 'e</w>', 'th', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 36\n",
      "==========\n",
      "Iter: 9\n",
      "Best pair: ('o', 'n')\n",
      "All tokens: dict_keys(['the</w>', 'f', 'u', 'l', 't', 'on', '</w>', 'c', 'o', 'n', 'y', 'g', 'r', 'an', 'd</w>', 'j', 's', 'a', 'i', 'd', 'in', 'v', 'e', 't</w>', 'p', 'm', 'e</w>', 'th', 's</w>', 'k', 'er', 'x', 'w', 'h', 'b', 'z', 'q'])\n",
      "Number of tokens: 37\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    print('Iter: {}'.format(i))\n",
    "    vocab, best = step_bpe(vocab)\n",
    "    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "    print('Best pair: {}'.format(best))\n",
    "    print('All tokens: {}'.format(tokens_frequencies.keys()))\n",
    "    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "J7jPGo103m63",
    "outputId": "0249c71c-0684-477a-926c-4e6ac607793c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the</w> 70025\n",
      "f 108098\n",
      "u 124718\n",
      "l 188286\n",
      "t 202416\n",
      "on 59265\n",
      "</w> 463087\n",
      "c 142440\n",
      "o 290817\n",
      "n 107396\n",
      "y 78777\n",
      "g 89353\n",
      "r 206423\n",
      "an 71278\n",
      "d</w> 103902\n",
      "j 7215\n",
      "s 174596\n",
      "a 301633\n",
      "i 248484\n",
      "d 77033\n",
      "in 87565\n",
      "v 45674\n",
      "e 302602\n",
      "t</w> 91543\n",
      "p 92998\n",
      "m 115244\n",
      "e</w> 130997\n",
      "th 63074\n",
      "s</w> 122162\n",
      "k 29857\n",
      "er 73869\n",
      "x 9052\n",
      "w 86528\n",
      "h 119094\n",
      "b 70812\n",
      "z 4316\n",
      "q 4977\n"
     ]
    }
   ],
   "source": [
    "#list(tokens_frequencies.keys())\n",
    "for tok, freq in tokens_frequencies.items():\n",
    "    print(tok,freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QEZMMuGF3m64",
    "outputId": "88db1c86-af3e-44de-e16d-51692fd7a7cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Best pair: ('y', '</w>')\n",
      "==========\n",
      "Iter: 100\n",
      "Best pair: ('f', 'or')\n",
      "==========\n",
      "Iter: 200\n",
      "Best pair: ('m', '</w>')\n",
      "==========\n",
      "Iter: 300\n",
      "Best pair: ('th', 'ing</w>')\n",
      "==========\n",
      "Iter: 400\n",
      "Best pair: ('o', 'f')\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "num_merges = 500\n",
    "for i in range(num_merges):\n",
    "    vocab, best = step_bpe(vocab)\n",
    "    if i % 100 == 0:\n",
    "        print('Iter: {}'.format(i))\n",
    "        print('Best pair: {}'.format(best))\n",
    "        print('==========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lE9LutHg3m65",
    "outputId": "33395ada-40e4-4a09-e6c0-c12ecb00b7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['through</w>', 'ations</w>', 'tional</w>', 'before</w>', 'ation</w>', 'which</w>', 'there</w>', 'would</w>', 'their</w>', 'other</w>']\n"
     ]
    }
   ],
   "source": [
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "\n",
    "print(sorted_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_6jBmLmv3m65",
    "outputId": "f2262da2-206c-46d8-d188-884940181f64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZWNsZqIP3m66",
    "outputId": "7219f14a-b432-4e19-8ebc-8e78b157d704"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m', 'oun', 'ta', 'in', 's</w>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenization.get(\"mountains</w>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Uzpudgor3m66",
    "outputId": "6555c94b-cb2a-41bd-d805-3a47328664fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f', 'ic', 'tional</w>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenization.get(\"fictional</w>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4CqscYMT3m66",
    "outputId": "3c43f3ee-9cce-453e-f2e6-2b1bbd4a77d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['li', 'v', 'el', 'in', 'ess</w>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenization.get(\"liveliness</w>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH_Z7xWr3m67"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
